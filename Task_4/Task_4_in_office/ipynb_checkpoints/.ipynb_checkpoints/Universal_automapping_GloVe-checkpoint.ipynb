{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zlc3zYOxxNM3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "#import scispacy\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VVVO-NI3y6gy"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\42324979.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read the CSV files into dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvoc2_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'target.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvoc1_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'to_map.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# voc1_df=voc1_df[:500]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Read the CSV files into dataframes\n",
    "\n",
    "voc2_df = pd.read_csv('target.csv')\n",
    "voc1_df = pd.read_csv('to_map.csv')\n",
    "# voc1_df=voc1_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ly_W5-3gys4z",
    "outputId": "fe007f9f-e5be-4a21-b6a6-747d9ed77f41"
   },
   "outputs": [],
   "source": [
    "voc1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ipiPSe65zA7R",
    "outputId": "7d52bee6-0657-4a7d-bd44-f7d45a669426"
   },
   "outputs": [],
   "source": [
    "voc2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LE3JZo-u1B34"
   },
   "outputs": [],
   "source": [
    "# words to remove from strings\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "             'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "             'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    "             'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "             'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "             'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "             'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "             'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "             'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "             'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "             'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain',\n",
    "             'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "             \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n",
    "             \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\",\n",
    "             'wouldn', \"wouldn't\",\n",
    "              # don't care  about right and left most of the time\n",
    "             #'right','left',\n",
    "             'noc', 'nos', '[d]', 'unknown_unit', '|', 'see comment', 'due', 'nec', 'unspecified', '[v]', '(see comments)',\n",
    "             # additional deletions for SNOMED\n",
    "              '(disorder)', '(procedure)', '(finding)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k21sZe6MVuK9",
    "outputId": "56332aa6-985f-4ef1-d9fa-ddbd674ca37b"
   },
   "outputs": [],
   "source": [
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz # en_core_sci_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "USSiuW8KO5g1"
   },
   "outputs": [],
   "source": [
    "# Загрузка модели для английского языка\n",
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5AZx21gOUGDC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voc1_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\445384526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ignores weird symbols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvoc1_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept_name_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoc1_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\s+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#voc2_df['concept_name_processed'] = voc2_df['concept_name'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x.encode('ascii', 'ignore').decode())))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Remove punctuation, digits etc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvoc1_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept_name_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoc1_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept_name_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[,\\.!;?)%(\\'\\\":\\-]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'voc1_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ignores weird symbols\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x).encode('ascii', 'ignore').decode()) if isinstance(x, str) else str(x))\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x.encode('ascii', 'ignore').decode())))\n",
    "# Remove punctuation, digits etc\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].map(lambda x: re.sub('[,\\.!;?)%(\\'\\\":\\-]', '', x))\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].map(lambda x: re.sub('[,\\.!;?)%(\\'\\\":\\-]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].map(lambda x: x.lower())\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].map(lambda x: x.lower())\n",
    "# Remove stopwords\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T40LH33YjtWa",
    "outputId": "3e892020-5817-4804-9dbb-4a53ae130def"
   },
   "outputs": [],
   "source": [
    "voc1_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "adGsTZn-FcIe",
    "outputId": "2c553d58-7eae-4f0c-ba8e-a3b71f8679aa"
   },
   "outputs": [],
   "source": [
    "voc2_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOc-mBIahXNN",
    "outputId": "34ecfefe-48b0-4963-9244-eb6cb77915d2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def tokenize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    return tokens\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(tokenize_with_spacy)\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(tokenize_with_spacy)\n",
    "\n",
    "elapsed_time = (time.time() - start_time)/60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE17joz72n6C",
    "outputId": "e61a3b18-1047-431c-cc14-30102eba6d92"
   },
   "outputs": [],
   "source": [
    "def lemmatize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Применение функции к вашему столбцу\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: lemmatize_with_spacy(' '.join(x)))\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: lemmatize_with_spacy(' '.join(x)))\n",
    "\n",
    "elapsed_time = (time.time() - start_time)/60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uQBEaOINlZBm",
    "outputId": "7b7221ab-7dac-41c4-ef6d-8d42f8a84c8f"
   },
   "outputs": [],
   "source": [
    "voc1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "c29UQE5hhBD0",
    "outputId": "79efcc8e-ee3c-433c-8c22-c1784390784c"
   },
   "outputs": [],
   "source": [
    "voc2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUhbWdnh2UdN"
   },
   "outputs": [],
   "source": [
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ')) # запускатЬ, если загружается преобработанный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJBzfxcKlabA"
   },
   "outputs": [],
   "source": [
    "# Save previously prepared vocabulary for future use\n",
    "# voc2_df.to_csv('/content/drive/MyDrive/SNOMED_OMOP_vocabulary_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Xoo83CA_2Vi1",
    "outputId": "04b9f1bf-287e-44a5-e47b-4981a5d79d2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_id</th>\n",
       "      <th>concept_name</th>\n",
       "      <th>concept_name_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36684768</td>\n",
       "      <td>Drusen of bilateral optic discs</td>\n",
       "      <td>[drusen, bilateral, optic, discs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37110249</td>\n",
       "      <td>Microvascular embolism of arteriole (disorder)</td>\n",
       "      <td>[microvascular, embolism, arteriole, disorder]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4220821</td>\n",
       "      <td>Bronzed diabetes</td>\n",
       "      <td>[bronzed, diabete]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4120412</td>\n",
       "      <td>Level of psychoticism</td>\n",
       "      <td>[level, psychoticism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4002835</td>\n",
       "      <td>Bruising of oropharynx</td>\n",
       "      <td>[bruise, oropharynx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564483</th>\n",
       "      <td>4036092</td>\n",
       "      <td>ROM - Range of motion activity</td>\n",
       "      <td>[rom, range, motion, activity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564484</th>\n",
       "      <td>37309624</td>\n",
       "      <td>Low back pain co-occurrent with left side scia...</td>\n",
       "      <td>[low, back, pain, cooccurrent, leave, side, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564485</th>\n",
       "      <td>4006806</td>\n",
       "      <td>Miscarriage with uraemia</td>\n",
       "      <td>[miscarriage, uraemia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564486</th>\n",
       "      <td>4238036</td>\n",
       "      <td>Urological fistula</td>\n",
       "      <td>[urological, fistula]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564487</th>\n",
       "      <td>4318113</td>\n",
       "      <td>Carbon monoxide poisoning from faulty furnace ...</td>\n",
       "      <td>[carbon, monoxide, poisoning, faulty, furnace,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564488 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        concept_id                                       concept_name  \\\n",
       "0         36684768                    Drusen of bilateral optic discs   \n",
       "1         37110249     Microvascular embolism of arteriole (disorder)   \n",
       "2          4220821                                   Bronzed diabetes   \n",
       "3          4120412                              Level of psychoticism   \n",
       "4          4002835                             Bruising of oropharynx   \n",
       "...            ...                                                ...   \n",
       "564483     4036092                     ROM - Range of motion activity   \n",
       "564484    37309624  Low back pain co-occurrent with left side scia...   \n",
       "564485     4006806                           Miscarriage with uraemia   \n",
       "564486     4238036                                 Urological fistula   \n",
       "564487     4318113  Carbon monoxide poisoning from faulty furnace ...   \n",
       "\n",
       "                                   concept_name_processed  \n",
       "0                       [drusen, bilateral, optic, discs]  \n",
       "1          [microvascular, embolism, arteriole, disorder]  \n",
       "2                                      [bronzed, diabete]  \n",
       "3                                   [level, psychoticism]  \n",
       "4                                    [bruise, oropharynx]  \n",
       "...                                                   ...  \n",
       "564483                     [rom, range, motion, activity]  \n",
       "564484  [low, back, pain, cooccurrent, leave, side, sc...  \n",
       "564485                             [miscarriage, uraemia]  \n",
       "564486                              [urological, fistula]  \n",
       "564487  [carbon, monoxide, poisoning, faulty, furnace,...  \n",
       "\n",
       "[564488 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LgsueZkC4WJk",
    "outputId": "9a2759ea-7c3d-49f5-fef4-db3dc63a15db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 concepts in 0.0 minutes\n",
      "Processed 10 concepts in 21.9 minutes\n",
      "Processed 20 concepts in 43.8 minutes\n",
      "Processed 30 concepts in 65.6 minutes\n",
      "Processed 40 concepts in 87.3 minutes\n",
      "Processed 50 concepts in 109.1 minutes\n",
      "Processed 60 concepts in 130.8 minutes\n",
      "Processed 70 concepts in 152.5 minutes\n",
      "Processed 80 concepts in 174.2 minutes\n",
      "Processed 90 concepts in 195.9 minutes\n",
      "Processed 100 concepts in 217.4 minutes\n",
      "Processed 110 concepts in 238.9 minutes\n",
      "Processed 120 concepts in 260.5 minutes\n",
      "Processed 130 concepts in 282.0 minutes\n",
      "Processed 140 concepts in 303.5 minutes\n",
      "Processed 150 concepts in 324.9 minutes\n",
      "Processed 160 concepts in 346.4 minutes\n",
      "Processed 170 concepts in 367.9 minutes\n",
      "Processed 180 concepts in 389.2 minutes\n",
      "Processed 190 concepts in 410.7 minutes\n",
      "Processed 200 concepts in 432.1 minutes\n",
      "Processed 210 concepts in 453.7 minutes\n",
      "Processed 220 concepts in 475.2 minutes\n",
      "Processed 230 concepts in 496.6 minutes\n",
      "Processed 240 concepts in 518.1 minutes\n",
      "Processed 250 concepts in 539.5 minutes\n",
      "Processed 260 concepts in 561.1 minutes\n",
      "Processed 270 concepts in 582.6 minutes\n",
      "Processed 280 concepts in 604.2 minutes\n",
      "Processed 290 concepts in 625.7 minutes\n",
      "Processed 300 concepts in 647.0 minutes\n",
      "Processed 310 concepts in 668.4 minutes\n",
      "Processed 320 concepts in 689.9 minutes\n",
      "Processed 330 concepts in 711.5 minutes\n",
      "Processed 340 concepts in 733.1 minutes\n",
      "Processed 350 concepts in 754.9 minutes\n",
      "Processed 360 concepts in 776.3 minutes\n"
     ]
    }
   ],
   "source": [
    "# Можно ли что-либо придумать с этим кодом?\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка предобученной модели GloVe\n",
    "glove_model_path = 'glove.6B.300d.txt'  # Путь к вашей модели GloVe\n",
    "glove_vectors = {}  # Словарь для хранения векторов слов\n",
    "\n",
    "with open(glove_model_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=float)\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "# Функция для векторизации текста с использованием GloVe\n",
    "def vectorize_text(text):\n",
    "    tokens = text.split()\n",
    "    vectors = [glove_vectors.get(token, np.zeros(300)) for token in tokens]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Усреднение векторов\n",
    "    else:\n",
    "        return np.zeros(300)  # Возвращает нулевой вектор, если нет известных слов\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "num_top_matches = 5  # Количество максимальных значений, которые вы хотите сохранить\n",
    "batch_size=10\n",
    "\n",
    "for voc1_index, voc1_row in voc1_df.iterrows():\n",
    "    if voc1_index % batch_size ==0:\n",
    "      elapsed_time=(time.time()-start_time)/60\n",
    "      print(f\"Processed {voc1_index} concepts in {elapsed_time:.1f} minutes\")\n",
    "\n",
    "\n",
    "    voc1_tokens = voc1_row['concept_name_processed']\n",
    "    voc1_name = voc1_row['concept_name']\n",
    "\n",
    "    voc1_text = ' '.join(voc1_tokens)\n",
    "    voc1_vector = vectorize_text(voc1_text)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for voc2_index, voc2_row in voc2_df.iterrows():\n",
    "\n",
    "\n",
    "\n",
    "        voc2_text = ' '.join(voc2_row['concept_name_processed'])\n",
    "        voc2_vector = vectorize_text(voc2_text)\n",
    "\n",
    "        similarity = cosine_similarity([voc1_vector], [voc2_vector])[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Находим индексы 5 наиболее похожих voc2(SNOMED и т. д.) концепций\n",
    "    top_indices = np.argsort(similarities)[-num_top_matches:][::-1]\n",
    "\n",
    "    for max_index in top_indices:\n",
    "        voc2_concept_id = voc2_df.loc[max_index, 'concept_id']\n",
    "        voc2_name = voc2_df.loc[max_index, 'concept_name']\n",
    "\n",
    "        # Добавляем отдельную строку информации в результаты\n",
    "        results.append((voc1_name, voc1_tokens, voc2_concept_id, voc2_name, max_index))\n",
    "\n",
    "# Конвертируем результаты в DataFrame\n",
    "columns = ['voc1_name', 'voc1_tokens', 'voc2_concept_id', 'voc2_name', 'max_index']\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpMQ0ah27DM-"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7q6eiThazSA"
   },
   "outputs": [],
   "source": [
    "# Output the results as a Pandas dataframe\n",
    "df_sorted = results_df.sort_values(by=['voc1_name', 'max_index'], ascending=False)\n",
    "df_sorted.to_csv('meddra_pt_mapping_results031023.csv', index=False)\n",
    "df_sorted.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzIPcvkVxkvf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
